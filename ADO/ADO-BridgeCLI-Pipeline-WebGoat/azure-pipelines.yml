# WebGoat Build, Security Scan, and Deployment Pipeline
# 
# Purpose: DevSecOps training demonstration showing comprehensive security scanning integration
# Architecture: 5-Stage Pipeline (Build → Container → Security → Deploy → Validate)
# Integration: Bridge CLI Direct Integration (CLI Switches) - NO Security Scan Task Extension
#
# Security Scans: 
# • SCA: package manager, signature, BDSC (Black Duck Secure Container) scanning via Bridge CLI
# • SAST: Coverity security vulnerability detection and code quality analysis via Bridge CLI
# • Coverage: Both PR and non-PR scans with full SARIF reporting
#
# Results appear in: Extensions tab, Tests tab, Pipeline artifacts, PR comments
# Gating: No build failures on security findings - builds succeed but marked "succeeded with issues"
#

trigger:
  branches:
    include:
    - main
    - develop
variables:
- group: blackduck-sca-variables
- group: coverity-variables
- group: bridge-cli-tokens
- name: PROJECT_NAME
  value: $(Build.Repository.Name)
- name: PROJECT_VERSION
  value: $(Build.SourceBranchName)
- name: K8S_SERVER_IP
  value: "172.31.17.121"
- name: K8S_PUBLIC_IP
  value: "44.253.226.227"
- name: WEBGOAT_NODEPORT
  value: "30080"
- name: WEBWOLF_NODEPORT
  value: "30090"
- name: COVERITY_VIEW
  value: 'Outstanding Issues'
- name: COVERITY_TOOL_HOME
  value: '/home/ubuntu/.blackduck/bridge/tools/cov-analysis/2025.6.2'
stages:
- stage: BuildWebGoat
  displayName: 'Stage 1: Build WebGoat'
  jobs:
  - job: BuildJob
    displayName: 'Build WebGoat Source Code'
    pool:
      name: 'Self-Hosted ADO Agent'
    steps:
    - task: CmdLine@2
      displayName: 'Verify Java 23 Environment'
      inputs:
        script: |
          set -ex
          echo "=== Java Environment Check ==="
          java -version
          javac -version
          echo "JAVA_HOME: $JAVA_HOME"
          ./mvnw -v
    - task: CmdLine@2
      displayName: 'Build WebGoat with Java 23'
      inputs:
        script: |
          set -ex
          echo "Building WebGoat from source with Java 23..."
          chmod +x ./mvnw
          ./mvnw clean install -DskipTests
          echo "=== Build artifacts ==="
          find . -name "webgoat-*.jar" -not -path "*/original/*" -ls
    - task: PublishPipelineArtifact@1
      displayName: 'Publish WebGoat Source and JAR'
      inputs:
        targetPath: '$(Build.SourcesDirectory)'
        artifact: 'WebGoatSource'
- stage: BuildContainer
  displayName: 'Stage 2: Build/Verify Container'
  dependsOn:
  - BuildWebGoat
  jobs:
  - job: ContainerBuildJob
    displayName: 'Build and Verify WebGoat Container'
    pool:
      name: 'Self-Hosted ADO Agent'
    steps:
    - task: DownloadPipelineArtifact@2
      displayName: 'Download WebGoat Source'
      inputs:
        artifact: 'WebGoatSource'
        path: '$(Build.SourcesDirectory)'
    - task: CmdLine@2
      displayName: 'Create WebGoat Dockerfile'
      inputs:
        script: |
          echo "Creating WebGoat Dockerfile..."
          JAR_FILE=$(find . -name "webgoat-*.jar" -not -path "*/original/*" | head -1)
          if [ -z "$JAR_FILE" ]; then
              echo "ERROR: No WebGoat JAR file found!"
              echo "Available JAR files:"
              find . -name "*.jar" -type f
              exit 1
          fi
          echo "Found JAR file: $JAR_FILE"

          cat > Dockerfile << 'EOF'
          FROM docker.io/eclipse-temurin:23-jdk-noble
          LABEL name="WebGoat: A deliberately insecure Web Application"
          LABEL maintainer="WebGoat team"
          LABEL version="BUILD_ID_PLACEHOLDER"
          RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*
          RUN useradd -ms /bin/bash webgoat && chgrp -R 0 /home/webgoat && chmod -R g=u /home/webgoat
          RUN mkdir -p /home/webgoat/.webgoat-2025.4-SNAPSHOT && chown -R webgoat:webgoat /home/webgoat
          USER webgoat
          COPY --chown=webgoat JAR_FILE_PLACEHOLDER /home/webgoat/webgoat.jar
          EXPOSE 8080
          EXPOSE 9090
          ENV TZ=Europe/Amsterdam
          WORKDIR /home/webgoat
          ENTRYPOINT [ "java", \
             "-Duser.home=/home/webgoat", \
             "-Dfile.encoding=UTF-8", \
             "--add-opens", "java.base/java.lang=ALL-UNNAMED", \
             "--add-opens", "java.base/java.util=ALL-UNNAMED", \
             "--add-opens", "java.base/java.lang.reflect=ALL-UNNAMED", \
             "--add-opens", "java.base/java.text=ALL-UNNAMED", \
             "--add-opens", "java.desktop/java.beans=ALL-UNNAMED", \
             "--add-opens", "java.desktop/java.awt.font=ALL-UNNAMED", \
             "--add-opens", "java.base/sun.nio.ch=ALL-UNNAMED", \
             "--add-opens", "java.base/java.io=ALL-UNNAMED", \
             "-Drunning.in.docker=true", \
             "-jar", "webgoat.jar" ]
          HEALTHCHECK --interval=10s --timeout=5s --start-period=60s --retries=5 \
             CMD curl --fail --silent http://localhost:8080/WebGoat/actuator/health || exit 1
          EOF

          sed -i "s|JAR_FILE_PLACEHOLDER|$JAR_FILE|g" Dockerfile
          sed -i "s|BUILD_ID_PLACEHOLDER|$(Build.BuildId)|g" Dockerfile
          echo "=== Created Dockerfile ==="
          cat Dockerfile
    - task: Docker@2
      displayName: 'Build WebGoat Docker Image'
      inputs:
        command: 'build'
        dockerfile: 'Dockerfile'
        repository: 'webgoat'
        tags: |
          $(Build.BuildId)
          latest
    - task: CmdLine@2
      displayName: 'Verify and Save Docker Image'
      inputs:
        script: |
          echo "=== Docker Image Verification ==="
          docker images | grep webgoat
          IMAGE_TAG="webgoat:$(Build.BuildId)"
          if docker images | grep -q "webgoat.*$(Build.BuildId)"; then
              echo "SUCCESS: Found $IMAGE_TAG"
          else
              echo "ERROR: $IMAGE_TAG not found!"
              exit 1
          fi

          # Save image as tar for deployment
          TAR_FILE="webgoat-$(Build.BuildId).tar"
          echo "Saving Docker image to tar file..."
          docker save ${IMAGE_TAG} -o ${TAR_FILE}
          if [ -f "${TAR_FILE}" ]; then
              TAR_SIZE=$(du -h ${TAR_FILE} | cut -f1)
              echo "SUCCESS: Created ${TAR_FILE} (${TAR_SIZE})"
          else
              echo "ERROR: Failed to create tar file"
              exit 1
          fi
    - task: PublishPipelineArtifact@1
      displayName: 'Publish Container Image'
      inputs:
        targetPath: 'webgoat-$(Build.BuildId).tar'
        artifact: 'WebGoatContainerImage'
- stage: SecurityScans
  displayName: 'Stage 3: Execute Security Scans (Bridge CLI)'
  dependsOn:
  - BuildWebGoat
  - BuildContainer
  jobs:
  - job: BlackDuckSourceScan
    displayName: 'Bridge CLI: Black Duck SCA Source Analysis'
    pool:
      name: 'Self-Hosted ADO Agent'
    steps:
    - task: DownloadPipelineArtifact@2
      displayName: 'Download WebGoat Source'
      inputs:
        artifact: 'WebGoatSource'
        path: '$(Build.SourcesDirectory)'
    - task: Bash@3
      env:
        BRIDGE_BLACKDUCKSCA_URL: $(BLACKDUCK_URL)
        BRIDGE_BLACKDUCKSCA_TOKEN: $(BLACKDUCK_API_TOKEN)
        BRIDGE_BLACKDUCKSCA_SCAN_FULL: true
        BRIDGE_BLACKDUCKSCA_FIXPR_ENABLED: true
        BRIDGE_BLACKDUCKSCA_REPORTS_SARIF_CREATE: true
        BRIDGE_AZURE_USER_TOKEN: $(AZURE_PERSONAL_ACCESS_TOKEN)
        BRIDGE_AZURE_ORGANIZATION_NAME: smiths0263
        BRIDGE_AZURE_REPOSITORY_NAME: $(Build.Repository.Name)
        BRIDGE_AZURE_PROJECT_NAME: $(System.TeamProject)
        BRIDGE_AZURE_REPOSITORY_BRANCH_NAME: $(Build.SourceBranchName)
        BRIDGE_AZURE_API_VERSION: "7.0"
        BRIDGE_AZURE_API_URL: "https://dev.azure.com"
        BRIDGE_BLACKDUCKSCA_FIXPR_MAXCOUNT: 5
        BRIDGE_BLACKDUCKSCA_FIXPR_FILTER_SEVERITIES: "CRITICAL,HIGH"
        DETECT_PROJECT_NAME: $(Build.Repository.Name)
        DETECT_PROJECT_VERSION_NAME: $(Build.SourceBranchName)
        DETECT_CODE_LOCATION_NAME: $(Build.Repository.Name)-$(Build.SourceBranchName)
        DETECT_BLACKDUCK_TIMEOUT: 900
        DETECT_API_TIMEOUT: 300000
        BLACKDUCK_TIMEOUT: 900
        INCLUDE_DIAGNOSTICS: 'true'
      displayName: 'Black Duck Full Scan'
      condition: not(eq(variables['Build.Reason'], 'PullRequest'))
      timeoutInMinutes: 45
      continueOnError: true
      inputs:
        targetType: inline
        script: |
          set -ex
          curl -fLsS -o bridge.zip $BRIDGECLI_LINUX64 && unzip -qo -d $(Agent.TempDirectory) bridge.zip && rm -f bridge.zip

          # Check Bridge CLI version for troubleshooting
          echo "=== Bridge CLI Version ==="
          $(Agent.TempDirectory)/bridge-cli-bundle-linux64/bridge-cli --version

          # Test connectivity first
          echo "Testing Black Duck connectivity..."
          curl -I $(BLACKDUCK_URL) --max-time 30 || echo "Warning: Connectivity test failed"

          # Run Bridge CLI with extended timeouts and enhanced Azure configuration
          timeout 1800 $(Agent.TempDirectory)/bridge-cli-bundle-linux64/bridge-cli --stage blackducksca \
            network.ssl.trustAll=true
    - task: Bash@3
      env:
        BRIDGE_BLACKDUCKSCA_URL: $(BLACKDUCK_URL)
        BRIDGE_BLACKDUCKSCA_TOKEN: $(BLACKDUCK_API_TOKEN)
        BRIDGE_BLACKDUCKSCA_SCAN_FULL: false
        BRIDGE_BLACKDUCKSCA_AUTOMATION_PRCOMMENT: true
        BRIDGE_AZURE_USER_TOKEN: $(AZURE_PERSONAL_ACCESS_TOKEN)
        BRIDGE_AZURE_ORGANIZATION_NAME: smiths0263
        BRIDGE_AZURE_REPOSITORY_NAME: $(Build.Repository.Name)
        BRIDGE_AZURE_PROJECT_NAME: $(System.TeamProject)
        BRIDGE_AZURE_REPOSITORY_BRANCH_NAME: $(System.PullRequest.SourceBranch)
        BRIDGE_AZURE_REPOSITORY_PULL_NUMBER: $(System.PullRequest.PullRequestId)
        BRIDGE_AZURE_API_VERSION: "7.0"
        BRIDGE_AZURE_API_URL: "https://dev.azure.com"
        DETECT_PROJECT_NAME: $(Build.Repository.Name)
        DETECT_PROJECT_VERSION_NAME: $(System.PullRequest.targetBranchName)
        DETECT_CODE_LOCATION_NAME: $(Build.Repository.Name)-$(System.PullRequest.targetBranchName)
        DETECT_BLACKDUCK_TIMEOUT: 900
        DETECT_API_TIMEOUT: 300000
        BLACKDUCK_TIMEOUT: 900
        INCLUDE_DIAGNOSTICS: 'true'
      displayName: 'Black Duck PR Scan'
      condition: eq(variables['Build.Reason'], 'PullRequest')
      timeoutInMinutes: 45
      continueOnError: true
      inputs:
        targetType: inline
        script: |
          set -ex
          curl -fLsS -o bridge.zip $BRIDGECLI_LINUX64 && unzip -qo -d $(Agent.TempDirectory) bridge.zip && rm -f bridge.zip

          # Check Bridge CLI version for troubleshooting
          echo "=== Bridge CLI Version ==="
          $(Agent.TempDirectory)/bridge-cli-bundle-linux64/bridge-cli --version

          # Test connectivity first
          echo "Testing Black Duck connectivity..."
          curl -I $(BLACKDUCK_URL) --max-time 30 || echo "Warning: Connectivity test failed"

          # Run Bridge CLI with extended timeouts
          timeout 1800 $(Agent.TempDirectory)/bridge-cli-bundle-linux64/bridge-cli --stage blackducksca
  - job: BlackDuckContainerScan
    displayName: 'Bridge CLI: Black Duck Container Analysis'
    pool:
      name: 'Self-Hosted ADO Agent'
    steps:
    - task: DownloadPipelineArtifact@2
      displayName: 'Download WebGoat Container Image'
      inputs:
        artifact: 'WebGoatContainerImage'
        path: '$(Build.SourcesDirectory)'
    - task: Bash@3
      env:
        BRIDGE_BLACKDUCKSCA_URL: $(BLACKDUCK_URL)
        BRIDGE_BLACKDUCKSCA_TOKEN: $(BLACKDUCK_API_TOKEN)
        BRIDGE_BLACKDUCKSCA_SCAN_FULL: true
        BRIDGE_BLACKDUCKSCA_REPORTS_SARIF_CREATE: true
        DETECT_PROJECT_NAME: $(Build.Repository.Name)
        DETECT_PROJECT_VERSION_NAME: $(Build.SourceBranchName)-container
        DETECT_CODE_LOCATION_NAME: $(Build.Repository.Name)-$(Build.SourceBranchName)-container
        DETECT_DOCKER_IMAGE: webgoat:$(Build.BuildId)
      displayName: 'Black Duck Container Scan'
      continueOnError: true
      inputs:
        targetType: inline
        script: |
          set -ex
          # Verify container image exists
          TAR_FILE="webgoat-$(Build.BuildId).tar"
          if [ ! -f "${TAR_FILE}" ]; then
              echo "ERROR: Container image tar file not found: ${TAR_FILE}"
              exit 1
          fi

          # Load image for scanning
          docker load -i ${TAR_FILE}

          curl -fLsS -o bridge.zip $BRIDGECLI_LINUX64 && unzip -qo -d $(Agent.TempDirectory) bridge.zip && rm -f bridge.zip
          $(Agent.TempDirectory)/bridge-cli-bundle-linux64/bridge-cli --stage blackducksca \
            network.ssl.trustAll=true
  - job: CoveritySAST
    displayName: 'Bridge CLI: Coverity SAST Analysis with SARIF'
    pool:
      name: 'Self-Hosted ADO Agent'
    steps:
    - task: DownloadPipelineArtifact@2
      displayName: 'Download WebGoat Source'
      inputs:
        artifact: 'WebGoatSource'
        path: '$(Build.SourcesDirectory)'
    - task: Bash@3
      env:
        BRIDGE_COVERITY_CONNECT_URL: $(COVERITY_URL)
        BRIDGE_COVERITY_CONNECT_USER_NAME: $(COV_USER)
        BRIDGE_COVERITY_CONNECT_USER_PASSWORD: $(COVERITY_PASSPHRASE)
        BRIDGE_COVERITY_CONNECT_PROJECT_NAME: $(Build.Repository.Name)
        BRIDGE_COVERITY_CONNECT_STREAM_NAME: $(Build.Repository.Name)-$(Build.SourceBranchName)
        BRIDGE_COVERITY_LOCAL: true
      displayName: 'Coverity Full Scan'
      condition: not(eq(variables['Build.Reason'], 'PullRequest'))
      continueOnError: true
      inputs:
        targetType: inline
        script: |
          set -ex
          curl -fLsS -o bridge.zip $BRIDGECLI_LINUX64 && unzip -qo -d $(Agent.TempDirectory) bridge.zip && rm -f bridge.zip
          $(Agent.TempDirectory)/bridge-cli-bundle-linux64/bridge-cli --stage connect \
            network.ssl.trustAll=true
    - task: Bash@3
      env:
        BRIDGE_COVERITY_CONNECT_URL: $(COVERITY_URL)
        BRIDGE_COVERITY_CONNECT_USER_NAME: $(COV_USER)
        BRIDGE_COVERITY_CONNECT_USER_PASSWORD: $(COVERITY_PASSPHRASE)
        BRIDGE_COVERITY_CONNECT_PROJECT_NAME: $(Build.Repository.Name)
        BRIDGE_COVERITY_CONNECT_STREAM_NAME: $(Build.Repository.Name)-$(System.PullRequest.targetBranchName)
        BRIDGE_COVERITY_AUTOMATION_PRCOMMENT: true
        BRIDGE_AZURE_USER_TOKEN: $(AZURE_PERSONAL_ACCESS_TOKEN)
        BRIDGE_AZURE_ORGANIZATION_NAME: smiths0263
        BRIDGE_AZURE_REPOSITORY_NAME: $(Build.Repository.Name)
        BRIDGE_AZURE_PROJECT_NAME: $(System.TeamProject)
        BRIDGE_AZURE_REPOSITORY_BRANCH_NAME: $(System.PullRequest.SourceBranch)
        BRIDGE_AZURE_REPOSITORY_PULL_NUMBER: $(System.PullRequest.PullRequestId)
        BRIDGE_COVERITY_LOCAL: true
      displayName: 'Coverity PR Scan'
      condition: eq(variables['Build.Reason'], 'PullRequest')
      continueOnError: true
      inputs:
        targetType: inline
        script: |
          set -ex
          curl -fLsS -o bridge.zip $BRIDGECLI_LINUX64 && unzip -qo -d $(Agent.TempDirectory) bridge.zip && rm -f bridge.zip
          $(Agent.TempDirectory)/bridge-cli-bundle-linux64/bridge-cli --stage connect
    - task: Bash@3
      displayName: 'Generate Coverity SARIF Report'
      condition: always()  # Run even if previous steps failed
      continueOnError: true  # Don't fail the job if SARIF generation fails
      inputs:
        targetType: inline
        script: |
          set -e
          echo "=== Generating Coverity SARIF Report ==="
          
          # Function to safely exit with message
          safe_exit() {
              echo "WARNING: $1"
              echo "SARIF generation skipped, but continuing pipeline..."
              exit 0  # Exit successfully to not break the pipeline
          }
          
          # Enhanced tool path discovery with version support
          COVERITY_TOOL_HOME=""
          
          # Method 1: Try Bridge CLI tools directory with version subdirectories
          if [ -d "/home/ubuntu/.blackduck/bridge/tools" ]; then
              # Look for versioned cov-analysis directories
              BRIDGE_COV_PATH=$(find /home/ubuntu/.blackduck/bridge/tools -name "cov-analysis" -type d | head -1)
              if [ -n "$BRIDGE_COV_PATH" ]; then
                  # Look for version subdirectories
                  VERSION_PATH=$(find "$BRIDGE_COV_PATH" -maxdepth 1 -type d -name "*.*.*" | sort -V | tail -1)
                  if [ -n "$VERSION_PATH" ] && [ -d "$VERSION_PATH/bin" ]; then
                      COVERITY_TOOL_HOME="$VERSION_PATH"
                      echo "Found Bridge CLI Coverity tools at: $COVERITY_TOOL_HOME"
                  elif [ -d "$BRIDGE_COV_PATH/bin" ]; then
                      # Fallback: direct bin directory
                      COVERITY_TOOL_HOME="$BRIDGE_COV_PATH"
                      echo "Found Bridge CLI Coverity tools (no version) at: $COVERITY_TOOL_HOME"
                  fi
              fi
          fi
          
          # Method 2: Fallback to configured path
          if [ -z "$COVERITY_TOOL_HOME" ] && [ -d "$(COVERITY_TOOL_HOME)" ]; then
              echo "Using configured Coverity tool home: $(COVERITY_TOOL_HOME)"
              COVERITY_TOOL_HOME="$(COVERITY_TOOL_HOME)"
          fi
          
          # Method 3: Search common locations
          if [ -z "$COVERITY_TOOL_HOME" ]; then
              for path in /opt/coverity/*/bin /usr/local/coverity/*/bin; do
                  if [ -d "$path" ]; then
                      COVERITY_TOOL_HOME=$(dirname "$path")
                      echo "Found Coverity tools at: $COVERITY_TOOL_HOME"
                      break
                  fi
              done
          fi
          
          # Validation: Check if we found valid Coverity tools
          if [ -z "$COVERITY_TOOL_HOME" ]; then
              safe_exit "No Coverity tools found in expected locations"
          fi
          
          echo "Using Coverity tools at: $COVERITY_TOOL_HOME"
          
          # Verify required executables exist
          if [ ! -f "$COVERITY_TOOL_HOME/bin/cov-format-errors" ]; then
              safe_exit "cov-format-errors not found at $COVERITY_TOOL_HOME/bin/"
          fi
          
          # Check for SARIF converter (try multiple possible locations)
          SARIF_CONVERTER=""
          for sarif_path in "$COVERITY_TOOL_HOME/SARIF/cov-format-sarif-for-github.js" "$COVERITY_TOOL_HOME/bin/cov-format-sarif.js"; do
              if [ -f "$sarif_path" ]; then
                  SARIF_CONVERTER="$sarif_path"
                  break
              fi
          done
          
          if [ -z "$SARIF_CONVERTER" ]; then
              safe_exit "SARIF converter not found at expected locations"
          fi
          
          echo "Found SARIF converter at: $SARIF_CONVERTER"
          
          # Find the intermediate directory created by Bridge CLI
          IDIR=$(find $(Build.SourcesDirectory)/.bridge -name idir 2>/dev/null | head -1)
          if [ -z "$IDIR" ]; then
              echo "WARNING: No Coverity intermediate directory found"
              echo "Bridge directory contents:"
              find $(Build.SourcesDirectory)/.bridge -type d 2>/dev/null || echo "No .bridge directory found"
              safe_exit "Could not locate Coverity intermediate directory"
          fi
          
          echo "Found Coverity idir: $IDIR"
          
          # Convert Coverity results to JSON
          echo "Converting Coverity results to JSON..."
          if ! $COVERITY_TOOL_HOME/bin/cov-format-errors --dir "$IDIR" --json-output-v10 coverity-issues.json; then
              safe_exit "Failed to run cov-format-errors"
          fi
          
          if [ ! -f "coverity-issues.json" ]; then
              safe_exit "Failed to generate coverity-issues.json"
          fi
          
          echo "Generated coverity-issues.json ($(wc -l < coverity-issues.json) lines)"
          
          # Convert JSON to SARIF using Coverity's converter
          echo "Converting JSON to SARIF..."
          NODE_CMD=""
          if [ -f "$COVERITY_TOOL_HOME/node/bin/node" ]; then
              NODE_CMD="$COVERITY_TOOL_HOME/node/bin/node"
          elif command -v node >/dev/null 2>&1; then
              NODE_CMD="node"
          else
              safe_exit "Node.js not found - required for SARIF conversion"
          fi
          
          if ! $NODE_CMD "$SARIF_CONVERTER" \
            --inputFile coverity-issues.json \
            --outputFile coverity-report.sarif \
            --githubUrl https://dev.azure.com \
            --repoName $(Build.Repository.Name) \
            --checkoutPath $(Build.Repository.Name) $(Build.SourcesDirectory) $(Build.SourceVersion); then
              safe_exit "Failed to convert JSON to SARIF"
          fi
          
          if [ ! -f "coverity-report.sarif" ]; then
              safe_exit "Failed to generate coverity-report.sarif"
          fi
          
          echo "Generated coverity-report.sarif ($(wc -l < coverity-report.sarif) lines)"
          
          # Create CodeAnalysisLogs.zip for Azure DevOps SARIF integration
          zip CodeAnalysisLogs.zip coverity-report.sarif
          
          echo "=== SARIF Generation Summary ==="
          echo "Issues JSON: $(ls -lh coverity-issues.json)"
          echo "SARIF Report: $(ls -lh coverity-report.sarif)"
          echo "Archive: $(ls -lh CodeAnalysisLogs.zip)"
          
          # Display summary of findings
          if command -v jq >/dev/null 2>&1; then
              echo "=== Coverity Findings Summary ==="
              jq -r '.runs[0].results | length as $count | "Total findings: \($count)"' coverity-report.sarif 2>/dev/null || echo "Could not parse SARIF for summary"
          fi
          
          echo "SARIF generation completed successfully!"
    - task: PublishBuildArtifacts@1
      displayName: 'Publish Coverity SARIF Report'
      condition: and(always(), succeeded())  # Run if SARIF generation succeeded
      continueOnError: true  # Don't fail job if publishing fails
      inputs:
        PathtoPublish: 'coverity-report.sarif'
        ArtifactName: 'CoveritySARIF'
        
    - task: PublishBuildArtifacts@1
      displayName: 'Publish Code Analysis Logs'
      condition: and(always(), succeeded())  # Run if SARIF generation succeeded
      continueOnError: true  # Don't fail job if publishing fails
      inputs:
        PathtoPublish: 'CodeAnalysisLogs.zip'
        ArtifactName: 'CodeAnalysisLogs'
        
    # Alternative: Publish any SARIF files that exist (more resilient)
    - task: CopyFiles@2
      displayName: 'Copy SARIF Files (if they exist)'
      condition: always()
      continueOnError: true
      inputs:
        SourceFolder: '$(Build.SourcesDirectory)'
        Contents: |
          **/*.sarif
          **/CodeAnalysisLogs.zip
          **/*coverity*issues*.json
        TargetFolder: '$(Build.ArtifactStagingDirectory)/sarif-reports'
        flattenFolders: true
        
    - task: PublishBuildArtifacts@1
      displayName: 'Publish All SARIF Reports'
      condition: always()
      continueOnError: true
      inputs:
        PathtoPublish: '$(Build.ArtifactStagingDirectory)/sarif-reports'
        ArtifactName: 'AllSARIFReports'

    # Summary step to report what was accomplished
    - task: Bash@3
      displayName: 'SARIF Generation Summary'
      condition: always()
      continueOnError: true
      inputs:
        targetType: inline
        script: |
          echo "=== SARIF Generation Results ==="
          
          # Check what files were created
          if [ -f "coverity-report.sarif" ]; then
              echo "✅ SARIF report generated successfully"
              if command -v jq >/dev/null 2>&1; then
                  ISSUE_COUNT=$(jq -r '.runs[0].results | length' coverity-report.sarif 2>/dev/null || echo "unknown")
                  echo "   📊 Issues found: $ISSUE_COUNT"
              fi
          else
              echo "❌ SARIF report not generated"
          fi
          
          if [ -f "CodeAnalysisLogs.zip" ]; then
              echo "✅ Code analysis logs archived"
          else
              echo "❌ Code analysis logs not archived"
          fi
          
          if [ -f "coverity-issues.json" ]; then
              echo "✅ JSON issues file generated"
          else
              echo "❌ JSON issues file not generated"
          fi
          
          # Show what artifacts are available
          echo ""
          echo "=== Available artifacts ==="
          find $(Build.ArtifactStagingDirectory) -type f 2>/dev/null | sort || echo "No staging directory found"
  - job: PublishSecurityResults
    displayName: 'Publish Security Scan Results'
    dependsOn:
    - BlackDuckSourceScan
    - BlackDuckContainerScan
    - CoveritySAST
    condition: always()
    pool:
      name: 'Self-Hosted ADO Agent'
    steps:
    - task: DownloadPipelineArtifact@2
      displayName: 'Download Coverity SARIF'
      condition: always()
      inputs:
        artifact: 'CoveritySARIF'
        path: '$(Build.SourcesDirectory)/sarif'
      continueOnError: true
    - task: DownloadPipelineArtifact@2
      displayName: 'Download Code Analysis Logs'
      condition: always()
      inputs:
        artifact: 'CodeAnalysisLogs'
        path: '$(Build.SourcesDirectory)/logs'
      continueOnError: true
    - task: PublishTestResults@2
      displayName: 'Publish SARIF Results'
      condition: always()
      inputs:
        testResultsFormat: 'VSTest'
        testResultsFiles: |
          **/*.sarif
          **/CodeAnalysisLogs.zip
        searchFolder: '$(Build.SourcesDirectory)'
        publishRunAttachments: true
        failTaskOnFailedTests: false
        testRunTitle: 'Security Scan Results (SARIF)'
      continueOnError: true
    - task: Bash@3
      displayName: 'Security Results Summary'
      condition: always()
      continueOnError: true
      inputs:
        targetType: inline
        script: |
          set -e
          echo "=== Security Scan Results Summary ==="
          echo "Build ID: $(Build.BuildId)"
          echo "Repository: $(Build.Repository.Name)"
          echo "Branch: $(Build.SourceBranchName)"
          echo "Commit: $(Build.SourceVersion)"
          echo ""

          # Check for SARIF files and summarize
          if find . -name "*.sarif" -type f | head -1 >/dev/null 2>&1; then
              echo "SARIF Reports Found:"
              find . -name "*.sarif" -type f -exec echo "  - {}" \; -exec wc -l {} \;
              
              # If jq is available, provide detailed summary
              if command -v jq >/dev/null 2>&1; then
                  for sarif_file in $(find . -name "*.sarif" -type f); do
                      echo ""
                      echo "Analysis of $sarif_file:"
                      jq -r '
                        .runs[]? | 
                        "  Tool: " + (.tool.driver.name // "Unknown") + 
                        " | Results: " + (.results | length | tostring) +
                        " | Rules: " + (.tool.driver.rules | length | tostring)
                      ' "$sarif_file" 2>/dev/null || echo "  Could not parse SARIF file"
                  done
              fi
          else
              echo "No SARIF files found"
          fi

          echo ""
          echo "=== Available Artifacts ==="
          find . -name "*.zip" -o -name "*.sarif" -o -name "*.json" | sort
- stage: DeployKubernetes
  displayName: 'Stage 4: Deploy via Kubernetes'
  dependsOn:
  - BuildContainer
  - SecurityScans
  condition: succeededOrFailed()
  jobs:
  - job: KubernetesDeployment
    displayName: 'Deploy WebGoat to Kubernetes'
    pool:
      name: 'Self-Hosted ADO Agent'
    steps:
    - task: DownloadSecureFile@1
      displayName: 'Download SSH Key'
      name: sshkey
      inputs:
        secureFile: 'steve-pem'
    - task: DownloadPipelineArtifact@2
      displayName: 'Download Container Image'
      inputs:
        artifact: 'WebGoatContainerImage'
        path: '$(Build.SourcesDirectory)'
    - task: CmdLine@2
      displayName: 'Transfer Image to Kubernetes'
      inputs:
        script: |
          set -e
          TAR_FILE="webgoat-$(Build.BuildId).tar"
          echo "=== Setting up SSH and transferring image ==="
          cp $(sshkey.secureFilePath) steve.pem
          chmod 400 steve.pem

          if [ ! -f "${TAR_FILE}" ]; then
              echo "ERROR: Container image tar file not found: ${TAR_FILE}"
              exit 1
          fi

          echo "Transferring ${TAR_FILE} to Kubernetes server..."
          scp -o StrictHostKeyChecking=no -i steve.pem ${TAR_FILE} ubuntu@$(K8S_SERVER_IP):/tmp/

          echo "Loading image on Kubernetes server..."
          ssh -o StrictHostKeyChecking=no -i steve.pem ubuntu@$(K8S_SERVER_IP) "
            sudo microk8s ctr image import /tmp/${TAR_FILE}
            echo 'Verifying image import...'
            sudo microk8s ctr images list | grep webgoat
            rm -f /tmp/${TAR_FILE}
            echo 'Image transfer completed!'
          "
    - task: CmdLine@2
      displayName: 'Deploy to Kubernetes'
      inputs:
        script: |
          IMAGE_NAME="docker.io/library/webgoat:$(Build.BuildId)"
          PUBLIC_IP="$(K8S_PUBLIC_IP)"
          WEBGOAT_PORT="$(WEBGOAT_NODEPORT)"
          WEBWOLF_PORT="$(WEBWOLF_NODEPORT)"

          cat > webgoat-deployment.yaml << EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: webgoat
            namespace: default
            labels:
              app: webgoat
              version: "$(Build.BuildId)"
              deployed-by: azure-devops
              scanned-by: bridge-cli
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: webgoat
            template:
              metadata:
                labels:
                  app: webgoat
                  version: "$(Build.BuildId)"
              spec:
                containers:
                - name: webgoat
                  image: ${IMAGE_NAME}
                  imagePullPolicy: Never
                  ports:
                  - containerPort: 8080
                    name: webgoat
                    protocol: TCP
                  - containerPort: 9090
                    name: webwolf
                    protocol: TCP
                  resources:
                    requests:
                      memory: "1Gi"
                      cpu: "500m"
                    limits:
                      memory: "2Gi"
                      cpu: "1"
                  env:
                  - name: TZ
                    value: "America/New_York"
                  - name: WEBGOAT_HOST
                    value: "${PUBLIC_IP}"
                  - name: WEBWOLF_HOST
                    value: "${PUBLIC_IP}"
                  command: ["java"]
                  args: [
                    "-Duser.home=/home/webgoat",
                    "-Dfile.encoding=UTF-8",
                    "--add-opens", "java.base/java.lang=ALL-UNNAMED",
                    "--add-opens", "java.base/java.util=ALL-UNNAMED",
                    "--add-opens", "java.base/java.lang.reflect=ALL-UNNAMED",
                    "--add-opens", "java.base/java.text=ALL-UNNAMED",
                    "--add-opens", "java.desktop/java.beans=ALL-UNNAMED",
                    "--add-opens", "java.desktop/java.awt.font=ALL-UNNAMED",
                    "--add-opens", "java.base/sun.nio.ch=ALL-UNNAMED",
                    "--add-opens", "java.base/java.io=ALL-UNNAMED",
                    "-Drunning.in.docker=true",
                    "-jar", "webgoat.jar",
                    "--server.address", "0.0.0.0"
                  ]
                  livenessProbe:
                    httpGet:
                      path: /WebGoat/actuator/health
                      port: 8080
                      scheme: HTTP
                    initialDelaySeconds: 180
                    periodSeconds: 30
                    failureThreshold: 5
                    timeoutSeconds: 15
                  readinessProbe:
                    httpGet:
                      path: /WebGoat/actuator/health
                      port: 8080
                      scheme: HTTP
                    initialDelaySeconds: 120
                    periodSeconds: 15
                    failureThreshold: 5
                    timeoutSeconds: 15
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: webgoat-service
            namespace: default
            labels:
              app: webgoat
          spec:
            selector:
              app: webgoat
            ports:
            - port: 8080
              targetPort: 8080
              protocol: TCP
              name: webgoat
              nodePort: ${WEBGOAT_PORT}
            - port: 9090
              targetPort: 9090
              protocol: TCP
              name: webwolf
              nodePort: ${WEBWOLF_PORT}
            type: NodePort
          EOF

          scp -o StrictHostKeyChecking=no -i steve.pem webgoat-deployment.yaml ubuntu@$(K8S_SERVER_IP):/tmp/

          ssh -o StrictHostKeyChecking=no -i steve.pem ubuntu@$(K8S_SERVER_IP) "
            echo '=== Cleaning up existing deployment ==='
            kubectl delete deployment webgoat --ignore-not-found=true --grace-period=30
            kubectl delete service webgoat-service --ignore-not-found=true --grace-period=15
            sleep 15
            
            echo '=== Applying WebGoat Deployment ==='
            kubectl apply -f /tmp/webgoat-deployment.yaml
            
            echo '=== Monitoring Deployment Rollout ==='
            kubectl rollout status deployment/webgoat --timeout=300s || echo 'Deployment in progress...'
            
            echo '=== Current Status ==='
            kubectl get deployment webgoat
            kubectl get service webgoat-service
            kubectl get pods -l app=webgoat -o wide
            
            rm -f /tmp/webgoat-deployment.yaml
          "
- stage: Validation
  displayName: 'Stage 5: Validation'
  dependsOn:
  - DeployKubernetes
  jobs:
  - job: ValidationTest
    displayName: 'Validate Deployment'
    pool:
      name: 'Self-Hosted ADO Agent'
    steps:
    - task: DownloadSecureFile@1
      displayName: 'Download SSH Key'
      name: sshkey
      inputs:
        secureFile: 'steve-pem'
    - task: CmdLine@2
      displayName: 'Validate Deployment and Application'
      inputs:
        script: |
          echo "==============================================="
          echo "WEBGOAT DEPLOYMENT VALIDATION"
          echo "Bridge CLI Security Integration Pipeline"
          echo "==============================================="

          cp $(sshkey.secureFilePath) steve.pem
          chmod 400 steve.pem

          ssh -o StrictHostKeyChecking=no -i steve.pem ubuntu@$(K8S_SERVER_IP) "
            echo '=== Pod Status Check ==='
            kubectl get pods -l app=webgoat -o wide
            
            echo '=== Waiting for Pod Readiness ==='
            kubectl wait --for=condition=ready pod -l app=webgoat --timeout=300s || echo 'Pod readiness timeout'
            
            POD_NAME=\$(kubectl get pods -l app=webgoat -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo 'NO_POD')
            
            if [ \"\$POD_NAME\" = 'NO_POD' ]; then
                echo 'CRITICAL: No WebGoat pod found!'
                exit 1
            fi
            
            echo \"Working with pod: \$POD_NAME\"
            
            echo '=== Application Health Check ==='
            for attempt in {1..6}; do
              echo \"Health check attempt \$attempt/6...\"
              if kubectl exec \$POD_NAME -- curl -s -f http://localhost:8080/WebGoat/actuator/health >/dev/null 2>&1; then
                echo 'SUCCESS: WebGoat health endpoint responding!'
                break
              elif [ \$attempt -eq 6 ]; then
                echo 'WebGoat health endpoint not responding after 6 attempts'
              else
                echo 'WebGoat not ready, retrying in 30 seconds...'
                sleep 30
              fi
            done
          "

          echo ""
          echo "External connectivity test..."
          sleep 30
          if timeout 25 curl -s -I http://$(K8S_PUBLIC_IP):$(WEBGOAT_NODEPORT)/WebGoat/ 2>/dev/null | head -1 | grep -q "HTTP"; then
              echo "SUCCESS: WebGoat is externally accessible!"
          else
              echo "Note: WebGoat may need more time to fully start"
          fi

          echo ""
          echo "==============================================="
          echo "         PIPELINE COMPLETION SUMMARY"
          echo "==============================================="
          echo ""
          echo "✅ Stage 1: Build WebGoat - COMPLETED"
          echo "✅ Stage 2: Build Container - COMPLETED"  
          echo "✅ Stage 3: Security Scans (Bridge CLI) - COMPLETED"
          echo "✅ Stage 4: Kubernetes Deployment - COMPLETED"
          echo "✅ Stage 5: Validation - COMPLETED"
          echo ""
          echo "Security Scans Executed:"
          echo "  ✅ Black Duck SCA Source Analysis"
          echo "  ✅ Black Duck Container Analysis"
          echo "  ✅ Coverity SAST Analysis with SARIF"
          echo ""
          echo "WebGoat Application Access:"
          echo "  Main: http://$(K8S_PUBLIC_IP):$(WEBGOAT_NODEPORT)/WebGoat/"
          echo "  Wolf: http://$(K8S_PUBLIC_IP):$(WEBWOLF_NODEPORT)/WebWolf/"
          echo ""
          echo "Security Reporting:"
          echo "  ✅ Black Duck SARIF reports (auto-generated)"
          echo "  ✅ Coverity SARIF reports (custom generated)"
          echo "  ✅ SARIF SAST Scans Tab integration"
          echo "  ✅ Black Duck Fix PR functionality (enhanced)"
          echo "==============================================="
